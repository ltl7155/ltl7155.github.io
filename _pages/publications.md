---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
redirect_from: 
  - /pubs
  - /pubs.html
  - /publications
  - /publications.html
---

You can also find my articles on <u><a href="https://scholar.google.com/citations?user=XB6CydwAAAAJ&hl=en">my Google Scholar profile</a>.</u> <i>(* for co-first author, # for corresponding author)</i>

---

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; <font color="blue">Tianlin Li</font>, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, Min Lin. Purifying Large Language Models by Ensembling a Small Language Model, <b>Preprint</b>.
</p>

<p>
<a class="media" href="https://openreview.net/forum?id=uOwJEPtyOF" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; <font color="blue">Tianlin Li*</font>, Xiaoyu Zhang*, Chao Du, Tianyu Pang, Qian Liu, Qing Guo, Chao
Shen, Yang Liu. Your Large Language Model is Secretly a Fairness Proponent and You
Should Prompt it Like One, <b>Preprint</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Yihao Huang, Yue Cao, <font color="blue">Tianlin Li#</font>, Felix Juefei-Xu, Di Lin, Ivor W Tsang, Yang
Liu, Qing Guo#. On the robustness of segment anything, <b>Preprint</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Xiaoyu Zhang, <font color="blue">Tianlin Li</font>, Cen Zhang, Yihao Huang, Xiaojun Jia, Xiaofei Xie,
Yang Liu, Chao Shen. A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection, <b>Preprint</b>.
</p>


<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Zhiming Li, Yanzhou Li, <font color="blue">Tianlin Li#</font>, Mengnan Du, Bozhi Wu, Yushi Cao, Xiaofei Xie, Yi Li, Yang Liu. Unveiling Project-Specific Bias in Neural Code Models, <b>LREC-Coling 2024</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Yanzhou Li, <font color="blue">Tianlin Li#</font>, Kangjie Chen#, Jian Zhang, Shangqing Liu, Wenhan
Wang, Tianwei Zhang, Yang Liu. BadEdit: Backdooring Large Language Models by Model Editing, <b>ICLR 2024</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/files/pubs/2023-ieeesp-rengar.pdf" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Yue Cao, <font color="blue">Tianlin Li*</font>, Xiaofeng Cao, Ivor Tsang, Yang Liu, Qing Guo. IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks, <b>ICLR 2024</b>.
</p>

<p>
<a class="media" href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-cen" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Zixin Yin*, Jiakai Wang*, Yisong Xiao, Hanqing Zhao, <font color="blue">Tianlin Li</font>, Wenbo Zhou, Aishan Liu, and Xianglong Liu. Improving Deepfake Detection Generalization by Invariant Risk Minimization, <b>TMM 2024</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/files/pubs/2022-issta-equafl.pdf" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Ming Hu, Yue Cao, Anran Li, Zhiming Li, Chengwei Liu, <font color="blue">Tianlin Li</font>, Mingsong Chen, Yang Liu. FedMut: Generalized Federated Learning via Stochastic Mutation, <b>AAAI 2024</b> <font color="red">oral</font>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/files/pubs/2021-ccs-ecmo.pdf" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Yihao Huang, Felix Juefei-Xu, Qing Guo, Jie Zhang, Yutong Wu, Hu Ming, <font color="blue">Tianlin Li</font>, Geguang Pu, Yang Liu. Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models, <b>AAAI 2024</b>.
</p>

<p>
<a class="media" href="https://www.usenix.org/conference/usenixsecurity21/presentation/zhang-cen" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Jian Zhang, Shangqing Liu, Xu Wang, <font color="blue">Tianlin Li</font>, Yang Liu. Learning to Locate and Describe Vulnerabilities, <b>ASE 2023</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/files/pubs/2021-ase-firmguide.pdf" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; <font color="blue">Tianlin Li*</font>, Yue Cao*, Jian Zhang, Shiqian Zhao, Yihao Huang, Aishan Liu, Qing Guo, Yang Liu. RUNNER: Responsible UNfair NEuron Repair for Enhancing Deep Neural Network Fairness, <b>ICSE 2024</b> <font color="red">oral</font>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; <font color="blue">Tianlin Li</font>, Xiaofei Xie, Jian Wang, Qing Guo, Aishan Liu, Lei Ma, Yang Liu. Faire: Repairing Fairness of Neural Networks via Neuron Condition Synthesis, <b>TOSEM 2023</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; <font color="blue">Tianlin Li</font>, Qing Guo, Aishan Liu, Mengnan Du, Zhiming Li, Yang Liu. FAIRER: FAIRNESS AS DECISION RATIONALE ALIGNMENT, <b>ICML 2023</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; <font color="blue">Tianlin Li</font>, Zhiming Li, Anran Li, Mengnan Du, Aishan Liu, Qing Guo, Guozhu
Meng, Yang Liu. Fairness via Group Contribution Matching, <b>IJCAI 2023</b> <font color="red">oral</font>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Yisong Xiao, Aishan Liu, <font color="blue">Tianlin Li</font>,Xianglong Liu. Latent Imitator: Generating Natural Individual Discriminatory, <b>ISSTA 2023</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp;  <font color="blue">Tianlin Li*</font>, Aishan Liu*, Xianglong Liu, Yitao Xu, Chongzhi Zhang, Xiaofei Xie. Understanding adversarial robustness via critical attacking route, <b>Information Science 2020</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Xiaofei Xie*, <font color="blue">Tianlin Li*</font>, Jian Wang, Lei Ma, Qing Guo, Felix Juefei-Xu, Yang Liu. Decision Logic of Deep Neural Networks, <b>TOSEM 2021</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Ruofan Liang*, <font color="blue">Tianlin Li*</font>, Longfei Li, Jing Wang, Quanshi Zhang. Knowledge consistency between neural networks, <b>ICLR 2020</b>.
</p>

<p>
<a class="media" href="https://ltl7155.github.io/404.html" target="_blank"><img src="https://ltl7155.github.io/images/pdf.png"></a>&nbsp; Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, <font color="blue">Tianlin Li</font>. Interpreting and improving adversarial robustness of deep neural networks with neuron sensitivity, <b>TIP 2020</b>.
</p>

---
